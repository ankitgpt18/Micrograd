# Micrograd - A Minimalistic Automatic Differentiation Library

Micrograd is a minimalistic automatic differentiation library implemented from scratch to understand the fundamentals of backpropagation and gradient descent. This project focuses on the core concepts of neural network training and optimization, such as the chain rule and gradient computation, which are essential for building and training deep learning models.

### Technologies:
- **Python**
- **NumPy**

---

## Table of Contents
1. [Installation](#installation)
2. [Usage](#usage)
3. [Features](#features)
4. [Core Concepts](#core-concepts)
5. [Example](#example)
6. [Contributing](#contributing)
7. [License](#license)

---

## Installation

To install **Micrograd**, simply clone this repository and use it in your Python project.

```bash
git clone https://github.com/your-username/micrograd.git
cd micrograd
