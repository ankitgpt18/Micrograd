# Micrograd - A Minimalistic Automatic Differentiation Library

Micrograd is a minimalistic automatic differentiation library implemented from scratch to understand the fundamentals of backpropagation and gradient descent. This project focuses on the core concepts of neural network training and optimization, such as the chain rule and gradient computation, which are essential for building and training deep learning models.

### Technologies:
- **Python**
- **NumPy**
